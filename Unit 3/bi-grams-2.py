# code generated by ChatGPT
from nltk import bigrams
from nltk.tokenize import word_tokenize

from collections import Counter

from nltk.corpus import brown

# Utilize large corpus for training the bi-gram model
# retrieve the brown corpus, news category
# but we limit the number of characters, as to not create a "big" model
text = " ".join(brown.words(categories="news")[:500])

# Generate the bigram model
def bigram_probabilities(text):
    tokens = word_tokenize(text.lower())
    bigram_counts = Counter(bigrams(tokens))
    unigram_counts = Counter(tokens)

    bigram_probs = {bigram: count / unigram_counts[bigram[0]]
                for bigram, count in bigram_counts.items()}
    return bigram_probs

print(bigram_probabilities(text))

# Test bigram model

# Evaluate bigram model